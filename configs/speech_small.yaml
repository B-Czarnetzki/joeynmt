name: "my_experiment"

# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparameter settings.

speech: True                       # default: True

data:
    src: "txt"                     # src language: expected suffix of train audio feature file, default: txt
    trg: "en"                      # trg language: expected suffix of target language sentence file.
    #scale: "None"                 # normalize data, default: "None", other options: "norm", "mean", "unit_var", "all"
    train: "test/data/speechtoy/toy"          # training data
    dev: "test/data/speechtoy/toy"            # development data for validation
    test: "test/data/speechtoy/toy"           # test data for testing final model; optional
    level: "char"                   # segmentation level: either "word", "bpe" or "char" (Char highly recommended for speech)
    transpose_features: False       # U can transposoe the input features, depending on you input format
    lowercase: True                 # lowercase the data, also for validation
    max_sent_length: 150            # filter out longer sentences from training
    max_audio_length: 1000          # filter out longer audio files from training
    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary
    src_voc_limit: 101              # src vocabulary only includes this many most frequent tokens, default: unlimited
    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary
    trg_voc_limit: 102              # trg vocabulary only includes this many most frequent tokens, default: unlimited
    #src_vocab: "my_model/src_vocab.txt"  # if specified, load a vocabulary from this file
    #trg_vocab: "my_model/trg_vocab.txt"  # one token per line, line number is index
    input_length_ratio: 100         # a possible training set's filter due to the length difference

testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)
    beam_size: 5                    # size of the beam for beam search
    alpha: 1.0                      # length penalty for beam search

training:                           # specify training details here
    #load_model: "my_model/3300.ckpt" # if given, load a pre-trained model from this checkpoint
    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.
    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.
    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.          # set this seed to make training deterministic
    random_seed: 77                 # set this seed to make training deterministic
    optimizer: "adam"               # choices: "sgd", "adam", "adadelta", "adagrad", "rmsprop", default is SGD
    learning_rate: 0.001            # initial learning rate, default: 3.0e-4
    learning_rate_min: 0.00001      # stop learning when learning rate is reduced below this threshold, default: 1.0e-8
    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional
    #clip_grad_norm: 1.0            # norm clipping instead of value clipping
    #weight_decay: 0.1              # l2 regularization, default: 0
    batch_size: 10                  # mini-batch size, required
    batch_type: "sentence"          # create batches with sentences ("sentence", default) or tokens ("token")
    eval_batch_size: 10             # mini-batch size for evaluation (see batch_size above)
    eval_batch_type: "sentence"     # evaluation batch type ("sentence", default) or tokens ("token")
    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches
    normalization: "batch"          # loss normalization of a mini-batch, default: "batch" (by number of sequences in batch), other options: "tokens" (by number of tokens in batch), "none" (don't normalize, sum up loss)
    scheduling: "plateau"           # learning rate scheduling, optional, if not specified stays constant, options: "plateau", "exponential", "decaying", "noam" (for Transformer), "warmupexponentialdecay"
    patience: 5                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate
    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor
    epochs: 10                      # train for this many epochs
    validation_freq: 5              # validate after this many updates (number of mini-batches), default: 1000
    logging_freq: 10                # log the training progress after this many updates, default: 100
    eval_metric: "cer"              # validation metric, default: "bleu", other options: "wer", "cer", "chrf", "token_accuracy", "sequence_accuracy"
    early_stopping_metric: "loss"   # when a new high score on this metric is achieved, a checkpoint is written, when "eval_metric" (default) is maximized, when "loss" or "ppl" is minimized
    model_dir: "models/mini/speechtoy" # directory where models and validation results are stored, required
    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!
    shuffle: False                  # shuffle the training data, default: True
    use_cuda: False                 # use CUDA for acceleration on GPU, required. Set to False when working on CPU.
    max_output_length: 150          # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length
    print_valid_sents: [0, 3, 5]    # print this many validation sentences during each validation run, default: 3
    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5

model:                              # specify your model architecture here
    tied_embeddings: False          # tie src and trg embeddings, only applicable if vocabularies are the same, default: False
    encoder:
        rnn_type: "lstm"            # type of recurrent unit to use, either "gru" or "lstm", default: "lstm"
        embeddings:
            embedding_dim: 41       # Must be number of audio features
            scale: False            # scale the embeddings by sqrt of their size, default: False
            freeze: True            # if True, embeddings are not updated during training, must be true for speech processing
            dropout: 0.0            # Apply dropout to the audio features
        hidden_size: 256            # size of RNN
        linear_hidden_size_1: 256   # size of first input linear layer
        linear_hidden_size_2: 128   # size of second linear layer
        bidirectional: True         # use a bi-directional encoder, default: True
        rnn_layer_dropout: 0.2      # apply dropout to the rnn layer outputs if num_layers > 1 (only applies to num_layers-1 not the last, default: 0.0
        rnn_input_dropout: 0.2      # apply dropout to the rnn input default: 0.0
        variational_dropout: True   # Use variational dropout (Highly recommended for speech processing)
        input_layer_dropout: 0.2    # apply dropout after each input linear layer, default: 0.0
        num_layers: 3               # stack this many layers of equal size, default: 1
        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)
        activation: "tanh"          # activation type for 2 layers following the src embeddings (only for speech), default: "relu", other options: "tanh"
        #bidir_projection: True     # Projects bidir encoder outputs (2*hidden_size) to encoder outputs size = hidden_size, via a linear layer (only use if decoder init_hidden=bridge)
        layer_norm: False           # layer normalization layers for linear layers and RNN layer (only if variationoal dropout: True), default: False
        emb_norm: False             # layer normalization layers for embeddings, default: False
    decoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 20
            scale: False
            freeze: False           # if True, embeddings are not updated during training
        hidden_size: 50
        dropout: 0.2
        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0
        init_state_dropout: 0.2     # dropout bridge_layer input (only used if init_hidden=bridge)
        num_layers: 1               # Num layers for both decoders of the ConoitionalRecurrentDecoder used for speech processing
        input_feeding: False        # combine hidden state and attention vector before feeding to rnn, default: True
        init_hidden: "bridge"       # initialized the decoder hidden state: use linear projection of last encoder state ("bridge") or simply the last state ("last") or zeros ("zero"), default: "bridge"
        attention: "bahdanau"       # attention mechanism, choices: "bahdanau" (MLP attention), "luong" (bilinear attention), default: "bahdanau"
        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)
